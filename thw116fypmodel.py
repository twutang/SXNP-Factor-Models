# -*- coding: utf-8 -*-
"""thw116FYPmodel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IHPEI-sPNSS0rKdFThywR4LTpeOt0Z_y
"""

import numpy as np
import pandas as pd
import datetime as dt

import matplotlib.dates as mdates
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import MinMaxScaler, Normalizer
from scipy import stats

from keras.models import Sequential, Model, load_model
from keras.layers import Activation, Dense, LSTM, Dropout, Masking, Input
from keras import optimizers
from keras.utils.vis_utils import model_to_dot

from IPython.display import SVG

from google.colab import files

import os

"""# Initial Data Importation and Structuring

## Accessing and loading data from Google Drive
"""

# mounting Google Drive which contains the relevant data

from google.colab import drive
drive.mount('/content/drive')

# Fundamental input data
ltm_book = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_book.csv')
ltm_div = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_div.csv')
ltm_ebit = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_ebit.csv')
ltm_eps = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_eps.csv')
ltm_fcf = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_fcf.csv')
ltm_pbook = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_pbook.csv')
ltm_sales = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ltm_sales.csv')

ntm_book = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_book.csv')
ntm_div = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_div.csv')
ntm_ebit = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_ebit.csv')
ntm_eps = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_eps.csv')
ntm_fcf = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_fcf.csv')
ntm_pbook = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_pbook.csv')
ntm_sales = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/ntm_sales.csv')

# Technical input data

price_high = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/price_high.csv')
price_low = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/price_low.csv')
price_open = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/price_open.csv')
volume = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/volume.csv')
enterprise_val = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/enterprise_val.csv')
market_cap = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/market_cap.csv')

print(ltm_book.head())
print("**************************************")
print(ltm_book.info())

# Price target output data

price_close = pd.read_csv('/content/drive/My Drive/thw116_FYP/data/price_close.csv')

print(price_close.head())
print("**************************************")
print(price_close.info())

"""##Restructuring and Standardising the datasets"""

# Filtering incorrect datasets to suitable company subsets
industrials_subset = price_close.columns.values.tolist()

ltm_inputs = [ltm_book, ltm_div, ltm_ebit, ltm_eps, ltm_fcf, ltm_pbook, ltm_sales] 
ntm_inputs = [ntm_book, ntm_div, ntm_ebit, ntm_eps, ntm_fcf, ntm_pbook, ntm_sales]
tech_inputs = [price_high, price_low, price_open, volume, enterprise_val, market_cap]

for i in range(0,len(ltm_inputs)):
  ltm_inputs[i] = ltm_inputs[i].loc[:, ltm_inputs[i].columns.str.contains('|'.join(industrials_subset))]
  ltm_inputs[i]['Unnamed: 0'] = price_close['Unnamed: 0']
  ltm_inputs[i].rename( columns={'Unnamed: 0':'Date'}, inplace=True )

for i in range(0,len(ntm_inputs)):
  ntm_inputs[i] = ntm_inputs[i].loc[:, ntm_inputs[i].columns.str.contains('|'.join(industrials_subset))]
  ntm_inputs[i]['Unnamed: 0'] = price_close['Unnamed: 0']
  ntm_inputs[i].rename( columns={'Unnamed: 0':'Date'}, inplace=True )

for i in range(0,len(tech_inputs)):
  tech_inputs[i] = tech_inputs[i].loc[:, tech_inputs[i].columns.str.contains('|'.join(industrials_subset))]
  tech_inputs[i]['Unnamed: 0'] = price_close['Unnamed: 0']
  tech_inputs[i].rename( columns={'Unnamed: 0':'Date'}, inplace=True )

  
price_close.rename( columns={'Unnamed: 0':'Date'}, inplace=True )

"""## Data Wrangling

In order to produce a dataframe suitable for the model, we must first take the raw *Price.csv*  file and convert it to returns. This can then be maniuplated further in a variety of ways to achieve the neccessary classification for variants of the model

###Date Index Generator
"""

# Conversion of TimeAndDate column to DatetimeIndex for ease of use

def time_index_generator(dataframe):
  dataframe['Date'] = pd.to_datetime(dataframe['Date'], dayfirst=True)
  dataframe.set_index('Date', inplace=True)


for i in range(0,len(ltm_inputs)):
  time_index_generator(ltm_inputs[i])
for i in range(0,len(ntm_inputs)):
  time_index_generator(ntm_inputs[i])
for i in range(0,len(tech_inputs)):
  time_index_generator(tech_inputs[i])  

time_index_generator(price_close)

# Check all columns align correctly
print(ltm_inputs[0].columns.difference(price_close.columns))
print(ltm_inputs[1].columns.difference(price_close.columns))
print(ltm_inputs[2].columns.difference(price_close.columns))
print(ltm_inputs[3].columns.difference(price_close.columns))

"""###Ticker Anonymisation"""

# Check that all tickers in the set match

# Helper function to generate ticker list
def ticker_list_generator(num):
  ticker_list = []
  for i in range(0, num):
    ticker_name = 'Ticker ' + str(i+1)
    ticker_list.append(ticker_name)
  return ticker_list

anon_tickers = ticker_list_generator(len(ltm_inputs[0].columns))

for i in range(0,len(ltm_inputs)):
  ltm_inputs[0].columns = anon_tickers
for i in range(0,len(ntm_inputs)):
  ntm_inputs[0].columns = anon_tickers
for i in range(0,len(tech_inputs)):
  tech_inputs[0].columns = anon_tickers

price_close.columns = anon_tickers

# # Selecting a Ticker and plotting

# ltm_inputs[0]['Ticker 1'].plot(figsize=(16,6))
# plt.title('ltm_book - Ticker 1')
# plt.xlabel('Date')
# plt.ylabel('Attribute')

"""###Price to Returns Converter"""

# Converting Price format to returns format
def price_to_returns(timeframe, dataframe):
  if timeframe == 'daily':
    return dataframe.pct_change(1) # remember to discount all target variables that are NaN
  
  if timeframe == 'monthly':
    return dataframe.resample('BM')
  
returns = price_to_returns('daily', price_close) 

print(returns.head(2))

"""# Data Preprocessing

Several steps are taken in order to structure the data so that it can be proccessed by the model.

1.   Data Analysis and Cleaning: Data is analysed for missing values, its properties, smoothed for various time intervals and resolved of any inconsistencies.  

2. Training-Test Split: Data is divided into training and test splits according to a selected parameter value. 

3.  Data Transformation: Data is normalised, aggregated and generalised, both for training and testing

4. Data Integration: Data is merged together appropriately to form the input shape of the model.

##Data Analysis

Something to consider is the fact that returns are more likely to correlate with changes in fundamentals rather than the absolute values of fundamentals. However, there is likely to be a disceprancy in the rate of change of fundamentals and the change in prices. Fundamentals are often only declared quarterly whereas prices are subject to daily fluctuations. We will first analyse the number of times a fundamental changes relative to the price changes.

###Stastical Summaries of Data Count and Boxplots
"""

# statistical summary of datasets
print(tech_inputs[0].describe())

# Box plot of dataset's datapoints count
tech_inputs[0].count().plot(kind='box', showmeans = True) 
plt.xlabel('Technical inputs')
plt.tight_layout() 
plt.show()

# plt.savefig('tech_box.png')
# files.download('tech_box.png') 

# Individual tickers' datapoint count bar graph

# ltm_inputs[0.count().plot(kind='barh', figsize=(20,20)) 
# plt.xlabel('LTM input')
# plt.tight_layout() 
# plt.show()

# plt.savefig('ltmTickerCount.png')
# files.download('ltmTickerCount.png')

"""###Stastical Summaries of Delta Data Count and Boxplots"""

def data_change(dataframe, column): 
  noChange_count = 0
  NaN_count = 0
  for i in range (0, len(dataframe.index)): 
    if dataframe[column].diff().iloc[i] == 0:
      noChange_count += 1
    if dataframe[column].isna().iloc[i]: 
      NaN_count += 1
  
#   print('Total rows: ', len(dataframe[column]))
#   print('The number of rows where no change occurs: ', noChange_count)
#   print('The number of rows which are NaN: ', NaN_count)
#   print('Useful datapoints:', (len(dataframe[column])-noChange_count-NaN_count))

  return len(dataframe[column])-noChange_count-NaN_count

delta_change = []

for companies in range(0, tech_inputs[0].shape[1]):
  delta_val = data_change(tech_inputs[0], anon_tickers[companies])
  delta_change.append(delta_val)
  
plt.boxplot(delta_change, showmeans = True)
plt.xlabel('Technical inputs')
plt.show()

"""### Returns distribution plots"""

# Plotting the distributions of the returns datapoints. 

allreturns = returns.stack(dropna=False).reset_index(drop=True).to_frame('Returns Plot')
allreturns.hist(range=(-0.2,0.2), figsize=(16,8), bins=200)

allreturns = np.log(allreturns) 
allreturns.hist(range=(-10,0), figsize=(16,8), bins=200)

"""## Data Snipping"""

# # Entire dataset
# for i in range(0,len(ltm_inputs)):
#   ltm_inputs[i] = ltm_inputs[i].loc[ltm_inputs[i].index > pd.to_datetime('2000-1-1')]

# for i in range(0,len(ntm_inputs)):
#   ntm_inputs[i] = ntm_inputs[i].loc[ntm_inputs[i].index > pd.to_datetime('2000-1-1')]

# for i in range(0,len(tech_inputs)):
#   tech_inputs[i] = tech_inputs[i].loc[tech_inputs[i].index > pd.to_datetime('2000-1-1')]
  
# returns = returns.loc[returns.index > pd.to_datetime('2000-1-1')]


# Reduced subset for faster training and testing

for i in range(0,len(ltm_inputs)):
  ltm_inputs[i] = ltm_inputs[i].loc[ltm_inputs[i].index > pd.to_datetime('2010-1-1')]

for i in range(0,len(ntm_inputs)):
  ntm_inputs[i] = ntm_inputs[i].loc[ntm_inputs[i].index > pd.to_datetime('2010-1-1')]

for i in range(0,len(tech_inputs)):
  tech_inputs[i] = tech_inputs[i].loc[tech_inputs[i].index > pd.to_datetime('2010-1-1')]
  
returns = returns.loc[returns.index > pd.to_datetime('2010-1-1')]

"""## Data splitting

### Training set
"""

def training_set(train_percentage, dataframe): 
  train_size = int(train_percentage*len(dataframe.index)) 
  train_set = dataframe[:train_size]
  return pd.DataFrame(train_set)  

training_split = 0.6

ltm_trainInputs = []
ntm_trainInputs = []
tech_trainInputs = []

for i in range(0,len(ltm_inputs)):
  ltm_trainInputs.append(training_set(training_split, ltm_inputs[i]))
  
for i in range(0,len(ntm_inputs)):
  ntm_trainInputs.append(training_set(training_split, ntm_inputs[i]))

for i in range(0,len(tech_inputs)):
  tech_trainInputs.append(training_set(training_split, tech_inputs[i]))

returns_trainOutput = training_set(training_split, returns)

print(ntm_trainInputs[0].info())
print(returns_trainOutput.info())

"""### Validation Set"""

def validation_set(train_percentage, val_percentage, dataframe): 
  train_size = int(train_percentage*len(dataframe.index)) 
  val_size = int(val_percentage*len(dataframe.index))
  # val_size = len(dataframe.index)-int(train_percentage*len(dataframe.index)) 
  val_set = dataframe[train_size:(train_size+val_size)]
  return pd.DataFrame(val_set)  

validation_split = 0.2

ltm_valInputs = []
ntm_valInputs = []
tech_valInputs = []

for i in range(0,len(ltm_inputs)):
  ltm_valInputs.append(validation_set(training_split, validation_split, ltm_inputs[i]))
  
for i in range(0,len(ntm_inputs)):
  ntm_valInputs.append(validation_set(training_split, validation_split, ntm_inputs[i]))

for i in range(0,len(tech_inputs)):
  tech_valInputs.append(validation_set(training_split, validation_split, tech_inputs[i]))

  
returns_valOutput = validation_set(training_split, validation_split, returns)

print(ltm_valInputs[0].info())
print(returns_valOutput.info())

"""### Test Set"""

def test_set(train_percentage, val_percentage, dataframe): 
  test_percentage = 1-train_percentage-val_percentage
  train_size = int(train_percentage*len(dataframe.index)) 
  val_size = int(val_percentage*len(dataframe.index))
  # val_size = len(dataframe.index)-int(train_percentage*len(dataframe.index)) 
  test_set = dataframe[(train_size+val_size):]
  return pd.DataFrame(test_set) 

ltm_testInputs = []
ntm_testInputs = []
tech_testInputs = []

for i in range(0,len(ltm_inputs)):
  ltm_testInputs.append(test_set(training_split, validation_split, ltm_inputs[i]))
  
for i in range(0,len(ntm_inputs)):
  ntm_testInputs.append(test_set(training_split, validation_split, ntm_inputs[i]))

for i in range(0,len(tech_inputs)):
  tech_testInputs.append(test_set(training_split, validation_split, tech_inputs[i]))

returns_testOutput = test_set(training_split, validation_split, returns)

print(ltm_testInputs[0].info())
print(returns_testOutput.info())

fig = plt.figure()
fig.set_figheight(28)

plt.subplot(4, 1, 1)
sns.heatmap(returns_trainOutput.isnull(), cbar=False)

plt.subplot(4, 1, 2)
sns.heatmap(returns_valOutput.isnull(), cbar=False)

plt.subplot(4, 1, 3)
sns.heatmap(tech_trainInputs[1].isnull(), cbar=False)

plt.subplot(4, 1, 4)
sns.heatmap(tech_valInputs[1].isnull(), cbar=False)

# plt.savefig('heatmap.png')
# files.download('heatmap.png')

"""## Input and Output Transformation

###Scaling
"""

def input_scaling(dataframe):
  
  # This is the MinMax Scaling function 
  sc = MinMaxScaler(feature_range = (0, 1))
  scaled_input = sc.fit_transform(dataframe) # This is now an n-dimensional array type
  
#   transformer = Normalizer().fit(dataframe)
#   Normalizer(copy=True, norm='l2')
#   transformer.transform(X)
  
  return scaled_input

np.warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')

for i in range(0,len(ltm_inputs)):
  ltm_trainInputs[i] = input_scaling(ltm_trainInputs[i])
  ltm_valInputs[i] = input_scaling(ltm_valInputs[i])
  ltm_testInputs[i] = input_scaling(ltm_testInputs[i])
  
for i in range(0,len(ntm_inputs)):
  ntm_trainInputs[i] = input_scaling(ntm_trainInputs[i])
  ntm_valInputs[i] = input_scaling(ntm_valInputs[i])
  ntm_testInputs[i] = input_scaling(ntm_testInputs[i])

for i in range(0,len(tech_inputs)):
  tech_trainInputs[i] = input_scaling(tech_trainInputs[i])
  tech_valInputs[i] = input_scaling(tech_valInputs[i])
  tech_testInputs[i] = input_scaling(tech_testInputs[i])
  
# sample_input = [ltm_trainInputs[0][1000:1001][0][3],ntm_trainInputs[0][1000:1001][0][3], tech_trainInputs[0][1000:1001][0][3] ]
# print(sample_input)

"""### Classifier"""

def output_classifier(type ,dataframe): 
  
  # No adjustments
  if type == 'raw':
    scaled_dataframe = dataframe
  
  # Binary classifier where return>0 is +1, and return<0 is 0 labels
  if type == 'binary':
    pos_returns = dataframe.values > 0
    neg_returns = dataframe.values <= 0 
    scaled_dataframe = pd.DataFrame(np.select([pos_returns,neg_returns], [1,0], default='NaN'), index=dataframe.index, columns=dataframe.columns)

  return scaled_dataframe

returns_trainOutput = output_classifier('binary', returns_trainOutput)
returns_valOutput = output_classifier('binary', returns_valOutput)

raw_testOutput = output_classifier('raw', returns_testOutput)
returns_testOutput = output_classifier('binary', returns_testOutput)

print (returns_trainOutput.head(2))
print (returns_trainOutput.info())

print (returns_valOutput.head(2))
print (returns_valOutput.info())

"""## Data Integration"""

trainInput = []
trainTarget = []

valInput = []
valTarget = []

testInput = []
testTarget = []
rawtestTarget = []

# Check that the indices are of the same length 
if len(ltm_trainInputs[0]) != len(returns_trainOutput):
  print ('training length', len(ltm_trainInputs[0]))
  print ('target length', len(returns_trainOutput))
  assert False, "Incompatible dataframe index lengths!"

# Training Set
for company in range(0, len(ltm_trainInputs[0][:1][0])):
  
  for time_unit in range(0, len(ltm_trainInputs[0])): 
    input_unit = []
    
    for ltm_attribute in range(0, len(ltm_trainInputs)): 
      input_unit.append(ltm_trainInputs[ltm_attribute][time_unit:time_unit+1][0][company])
                        
    for ntm_attribute in range(0, len(ntm_trainInputs)):
      input_unit.append(ntm_trainInputs[ntm_attribute][time_unit:time_unit+1][0][company])
                        
    for tech_attribute in range(0, len(tech_trainInputs)): 
      input_unit.append(tech_trainInputs[tech_attribute][time_unit:time_unit+1][0][company])
                        
    trainInput.append(input_unit)
    
for company in range(0, len(returns_trainOutput.columns)):
  for time_unit in range(0, len(returns_trainOutput.index)): 
    trainTarget.append(returns_trainOutput[anon_tickers[company]][time_unit])
  
# Validation Set 
for company in range(0, len(ltm_valInputs[0][:1][0])):
  
  for time_unit in range(0, len(ltm_valInputs[0])): 
    input_unit = []
    
    for ltm_attribute in range(0, len(ltm_valInputs)): 
      input_unit.append(ltm_valInputs[ltm_attribute][time_unit:time_unit+1][0][company])
                        
    for ntm_attribute in range(0, len(ntm_trainInputs)):
      input_unit.append(ntm_valInputs[ntm_attribute][time_unit:time_unit+1][0][company])
                        
    for tech_attribute in range(0, len(tech_trainInputs)): 
      input_unit.append(tech_valInputs[tech_attribute][time_unit:time_unit+1][0][company])
                        
    valInput.append(input_unit)
    
for company in range(0, len(returns_valOutput.columns)):
  for time_unit in range(0, len(returns_valOutput.index)): 
    valTarget.append(returns_valOutput[anon_tickers[company]][time_unit])

# Test Set
for company in range(0, len(ltm_testInputs[0][:1][0])):
  
  for time_unit in range(0, len(ltm_testInputs[0])): 
    input_unit = []
    
    for ltm_attribute in range(0, len(ltm_testInputs)): 
      input_unit.append(ltm_testInputs[ltm_attribute][time_unit:time_unit+1][0][company])
                        
    for ntm_attribute in range(0, len(ntm_testInputs)):
      input_unit.append(ntm_testInputs[ntm_attribute][time_unit:time_unit+1][0][company])
                        
    for tech_attribute in range(0, len(tech_testInputs)): 
      input_unit.append(tech_testInputs[tech_attribute][time_unit:time_unit+1][0][company])
                        
    testInput.append(input_unit)
    
for company in range(0, len(returns_testOutput.columns)):
  for time_unit in range(0, len(returns_testOutput.index)): 
    testTarget.append(returns_testOutput[anon_tickers[company]][time_unit])

for company in range(0, len(raw_testOutput.columns)):
  for time_unit in range(0, len(raw_testOutput.index)): 
    rawtestTarget.append(raw_testOutput[anon_tickers[company]][time_unit])

print(trainInput[0:1])
print(len(trainInput))
print(trainTarget[0:1])
print(len(trainTarget))

print(valInput[0:1])
print(len(valInput))
print(valTarget[0:1])
print(len(valTarget))

print(testInput[0:1])
print(len(testInput))
print(testTarget[0:1])
print(len(testTarget))

"""### Remove redundant data"""

#239637
#269977
#166449

def remove_useless_inputs(inputList, outputList):
  resultInput = []
  resultOutput = []
  for i in range (0, len(inputList)):
    nancount = 0
    for val in range(0, len(inputList[i])):
      if str(inputList[i:i+1][0][val]) == 'nan': 
        nancount += 1
#     if nancount < len(inputList[i:i+1][0]): 
    if nancount < 1:
      resultInput.append(inputList[i])
      resultOutput.append(outputList[i])
      
  return resultInput, resultOutput

newtrainInput, newtrainTarget = remove_useless_inputs(trainInput, trainTarget)
newvalInput, newvalTarget = remove_useless_inputs(valInput, valTarget)
newtestInput, newtestTarget = remove_useless_inputs(testInput, testTarget)
newtestInput, newrawtestTarget = remove_useless_inputs(testInput, rawtestTarget)


print(newtrainInput[0:1])
print(len(newtrainInput))
print(newtrainTarget[0:1])
print(len(newtrainTarget))

# 239635
# 253107
# 166448

def remove_useless_outputs(inputList, outputList):
  resultInput = []
  resultOutput = []
  for i in range (0, len(outputList)):
    if str(outputList[i:i+1][0]).lower() != 'nan': 
      resultInput.append(inputList[i])
      resultOutput.append(outputList[i])
   
  if isinstance(resultOutput[0:1][0], str):
    for x in range (0, len(resultOutput)):
      resultOutput[x:x+1] = list(map(int, resultOutput[x:x+1][0]))
  print(type(resultOutput[0:1][0]))

  print(type(resultOutput[0:1][0]))
  return resultInput, resultOutput

trainInput, trainTarget = remove_useless_outputs(newtrainInput, newtrainTarget)
valInput, valTarget = remove_useless_outputs(newvalInput, newvalTarget)
testInput, testTarget = remove_useless_inputs(newtestInput, newtestTarget)
testInput, rawtestTarget = remove_useless_inputs(newtestInput, newrawtestTarget)


print(trainInput[0:1])
print(len(trainInput))
print(trainTarget[0:1])
print(len(trainTarget))
print(type(trainTarget[0:1]))

"""### Reshaping data"""

# Conversion to numpy array for improved memory, performance and functionality
trainInput, trainTarget = np.array(trainInput), np.array(trainTarget)
valInput, valTarget = np.array(valInput), np.array(valTarget)
testInput, testTarget = np.array(testInput), np.array(testTarget)

print(trainInput[0:1])

trainInput = np.reshape(trainInput, (trainInput.shape[0], trainInput.shape[1], 1))
valInput = np.reshape(valInput, (valInput.shape[0], valInput.shape[1], 1))
testInput = np.reshape(testInput, (testInput.shape[0], testInput.shape[1], 1))

print(trainInput[0:1])
print(trainInput.shape[0])
print(trainInput.shape[1])

print(trainInTarget[0:1])
print(trainInTarget.shape[0])

"""# Network Implementation

## Core Architecture

### Single Output LSTM
"""

data_dim = trainInput.shape[1]
timesteps = trainInput.shape[0]


# Sample Code
# model parameters:

def create_model(train_X, train_Y, data_dim):
  lstm_units = 1024
  
#   print('Build baseline binary model...')
#   model = Sequential()
#   model.add(Masking(mask_value=0., input_shape=(data_dim, 1)))
#   model.add(LSTM(lstm_units))
#   model.add(Dense(1, activation='sigmoid'))
#   model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
  
    
  print('Build stacked binary model')
  lstm_units = int(lstm_units/4)

  model = Sequential()
  model.add(Masking(mask_value=0., input_shape=(data_dim, 1)))
  model.add(LSTM(lstm_units, return_sequences=True))
  model.add(Dropout(rate=0.2))
  model.add(LSTM(lstm_units, return_sequences=True))
  model.add(Dropout(rate=0.2))
  model.add(LSTM(lstm_units, return_sequences=True))
  model.add(Dropout(rate=0.2))
  model.add(LSTM(lstm_units))
  model.add(Dense(1, activation='sigmoid'))
  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


  return(model)

baseline_model = create_model(trainInput, trainTarget, data_dim)
print(baseline_model.summary())
SVG(model_to_dot(baseline_model, show_shapes=True).create(prog='dot', format='svg'))

history = baseline_model.fit(trainInput, trainTarget, epochs = 30, batch_size = 1024, verbose = 1)
baseline_model.save('baseline.h5')

"""### Multi output LSTM"""

class fypNet: 
  @staticmethod
  def build_input_predictor(inputs, data_dim, lstm_units):
    lstm_units = int(lstm_units/8)

    inPred = Masking(mask_value=-1., input_shape = (data_dim, 1))(inputs)
    
    inPred = LSTM(lstm_units, return_sequences=True)(inputs)
    inPred = Dropout(rate=0.2)(inPred)
    inPred = LSTM(lstm_units, return_sequences=True)(inPred)
    inPred = Dropout(rate=0.2)(inPred)
    inPred = LSTM(lstm_units, return_sequences=True)(inPred)
    inPred = Dropout(rate=0.2)(inPred)
    inPred = LSTM(lstm_units)(inPred)    
    inPred = Dense(20)(inPred)
    
    result = Activation('softmax', name= 'inPred_result')(inPred)
    
    return result
    
  @staticmethod
  def build_output_predictor(inputs, data_dim, lstm_units): 
    lstm_units = int(lstm_units/4)
    
    outPred = LSTM(lstm_units, return_sequences=True)(inputs)
    outPred = Dropout(rate=0.2)(outPred)
    outPred = LSTM(lstm_units, return_sequences=True)(outPred)
    outPred = Dropout(rate=0.2)(outPred)
    outPred = LSTM(lstm_units, return_sequences=True)(outPred)
    outPred = Dropout(rate=0.2)(outPred)
    outPred = LSTM(lstm_units)(outPred)    
    outPred = Dense(1)(outPred)
    result = Activation('sigmoid', name= 'outPred_result')(outPred)

    return result
  
  @staticmethod
  def build(data_dim, lstm_units):
         
    input_shape = (data_dim, 1)
    inputs = Input(shape = input_shape)
   
    inputBranch = fypNet.build_input_predictor(inputs, data_dim, lstm_units)
    outputBranch = fypNet.build_output_predictor(inputs, data_dim, lstm_units)
    
    model = Model(inputs= inputs, outputs= [inputBranch, outputBranch])
    
    return model

data_dim = trainInput.shape[1]
timesteps = trainInput.shape[0]
lstm_units = 1024
num_epochs = 30
# initial_lr = 1e-3
batch_sizes = 1024

# initialize our fypNet multi-output network
model = fypNet.build(data_dim, lstm_units)
 
losses = {'inPred_result': 'mean_squared_error','outPred_result': 'binary_crossentropy'}
 
# initialize the optimizer and compile the model

print('Compiling model...')
# opt = Adam(lr=initial_lr, decay=initial_lr/epochs)
model.compile(optimizer='adam', loss=losses, metrics=['acc'])

# inPred = trainInput.shift(-1)
trainInTarget = np.roll(trainInput, -1)
trainValTarget = np.roll(valInput, -1)

# print(inPred.head())
print(model.summary())
SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))

trainInTarget = np.reshape(trainInTarget, (trainInTarget.shape[0], trainInTarget.shape[1]))
trainValTarget = np.reshape(trainValTarget, (trainValTarget.shape[0], trainValTarget.shape[1]))

history = model.fit(trainInput,{"inPred_result": trainInTarget, "outPred_result": trainTarget}, validation_data=(valInput, {"inPred_result": trainValTarget, "outPred_result": valTarget}), batch_size = batch_sizes, epochs=num_epochs, verbose=1)
# Input prediction is the input vector shifted by 1

model.save('multioutput.h5')

history.history

print('hi')

"""#Results Interpretations

## Model Loading
"""

trained_stack = load_model('/content/drive/My Drive/thw116_FYP/saved_models/stacked_model.h5')

# trained_stack = load_model('/content/drive/My Drive/thw116_FYP/saved_models/multioutput.h5')
                           
stack_pred = trained_stack.predict(testInput, verbose=1)
print(stack_pred)

stack_classPred = trained_stack.predict_classes(testInput, verbose=1)
print(stack_classPred)

"""## Evaluation Metrics"""

def evaluation_metrics(prediction, actual):
  TP = 0
  TN = 0
  FP = 0
  FN = 0
  for i in range (0, len(actual)):
    if actual[i][0] == '1':
      if prediction[i][0] == 1:
        TP += 1
      else:
        FN += 1
    if actual[i][0] == '0':
      if prediction[i][0] == 0:
        TN += 1
      else: 
        FP += 1
     
  print(TP, TN, FP, FN)
  return (TP+TN)/(TP+TN+FP+FN), TP/(TP+FP), TP/(TP+FN)

accuracy, precision, recall = evaluation_metrics(stack_classPred, testTarget)

print(accuracy, precision, recall)

selected = []
selectedpred = []
originalret = []

notselectedpred = []
originalret2 = []

for i in range(len(rawtestTarget)):
  if stack_classPred[i][0] == 1: 
    selected.append(rawtestTarget[i])
    selectedpred.append(stack_pred[i][0])
    originalret.append(rawtestTarget[i])
  else:
    notselectedpred.append(stack_pred[i][0])
    originalret2.append(rawtestTarget[i])

plt.figure(figsize=(10, 10))
plt.title('Selected Prediction Probabilities vs Actual Return')
plt.ylabel('Actual Return')
plt.xlabel('Prediction Probabilites')

plt.scatter(selectedpred, originalret)

plt.figure(figsize=(10, 10))
plt.title('Not Selected Prediction Probabilities vs Actual Return')
plt.ylabel('Actual Return')
plt.xlabel('Prediction Probabilites')

plt.scatter(notselectedpred, originalret2)

"""## Selected Labels Results distributions"""

plt.hist(selected,range=(-0.2,0.2), bins=200)
plt.ylabel('Returns')

npselected = np.array(selected)
selectreturns = np.log(npselected) 
plt.hist(selectreturns, range=(-10,0), bins=200)

"""## Test Data Returns Distributions"""

plt.hist(rawtestTarget,range=(-0.2,0.2), bins=200)
plt.ylabel('Returns')

nprawtestTarget = np.array(rawtestTarget)
rawreturns = np.log(nprawtestTarget) 
plt.hist(rawreturns, range=(-10,0), bins=200)

"""## Benchmark

### Selected Label Statistics
"""

print(stats.describe(selected))
plt.boxplot(selected)
plt.tight_layout() 
plt.show()

"""### Test Data Statistics"""

print(stats.describe(rawtestTarget))
plt.boxplot(rawtestTarget)
plt.tight_layout() 
plt.show()

"""### Whole Dataset Statistics"""

allreturns = returns.stack(dropna=False).reset_index(drop=True).to_frame('Returns Plot')
listreturns = []
for i in range (len(allreturns.values)):
  listreturns.append(allreturns.values[i][0])

cleanedreturns = [x for x in listreturns if str(x) != 'nan']
  
print(stats.describe(cleanedreturns))
plt.boxplot(cleanedreturns)
plt.tight_layout() 
plt.show()

"""# Testing

## Unit Tests
"""

# Returns Converter Test

price = [1, 1.1, 1.21, 1.452]
change1 = [float('nan'), 0.1, 0.1, 0.2] # success case
change2 = [1, 0.2, 0.2, 0.2] # fail case
testdf1 = pd.DataFrame(price)
benchmarkdf1 = pd.DataFrame(change1)
benchmarkdf2 = pd.DataFrame(change2)

def test_RETURNS_CONVERTER(test, benchmark):
  results = price_to_returns('daily', test)
  pd.testing.assert_frame_equal(results, benchmark)
  print('test successful')

test_RETURNS_CONVERTER(testdf1, benchmarkdf1)
test_RETURNS_CONVERTER(testdf1, benchmarkdf2)

# Test Set Generator Test

initialframe = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
outputframe1 = [7, 8, 9, 10] # success case
outputframe2 = [4, 5, 6, 7] # fail case
outputframe3 = [6, 7, 8, 9, 10] # fail case

testdf1 = pd.DataFrame(initialframe)
benchmarkdf1 = pd.DataFrame(outputframe1)
benchmarkdf1.index += 6
benchmarkdf2 = pd.DataFrame(outputframe2)
benchmarkdf2.index += 6
benchmarkdf3 = pd.DataFrame(outputframe3)
benchmarkdf3.index += 6

def test_SET_GENERATOR(test, benchmark):
  results = test_set(0.5, 0.1, test)
  pd.testing.assert_frame_equal(results, benchmark)
  print('test successful')

test_SET_GENERATOR(testdf1, benchmarkdf1)
test_SET_GENERATOR(testdf1, benchmarkdf2)
test_SET_GENERATOR(testdf1, benchmarkdf3)

# Input Scaling Test
# for minmax case

inputvals = [1, 2, 3, 4, 5]

minmax1 = np.array([0, 0.25, 0.5, 0.75, 1]) # success case
minmax1 = np.reshape(minmax1,(5, 1))
minmax2 = np.array([1, 0.25, 0.75, 0.75, 0]) # fail case
minmax2 = np.reshape(minmax2,(5, 1))

testdf1 = pd.DataFrame(inputvals)

def test_INPUT_SCALER(test, benchmark):
  results = input_scaling(test)
  np.testing.assert_array_equal(results, benchmark)
  print('test successful')

test_INPUT_SCALER(testdf1, minmax1)
test_INPUT_SCALER(testdf1, minmax2)

# Output Classifier Test
# for zero or less = 0 and positive return = 1

returnvals = [0.5, 0.5, -0.2, -0.3, 0.8]
classifier1 = [str(1), str(1), str(0), str(0), str(1)] # success case
classifier2 = [str(0), str(0), str(0), str(0), str(0)] # fail case

testdf1 = pd.DataFrame(returnvals)
benchmarkdf1 = pd.DataFrame(classifier1)
benchmarkdf2 = pd.DataFrame(classifier2)

def test_OUTPUT_CLASSIFIER(test, benchmark):
  results = output_classifier('binary', test)
  pd.testing.assert_frame_equal(results, benchmark)
  print('test successful')

test_OUTPUT_CLASSIFIER(testdf1, benchmarkdf1)
test_OUTPUT_CLASSIFIER(testdf1, benchmarkdf2)

"""## Integration Tests"""

#Â Data Wrangling Test

start = dt.datetime.strptime("26-12-1999", "%d-%m-%Y")
end = dt.datetime.strptime("5-01-2000", "%d-%m-%Y")
dates = [start + dt.timedelta(days=x) for x in range(0, (end-start).days)]
values1 = [10000, 10000, 9000, 9000, 9900, 9900, 9999, 9999, 19998, 19998]
values2 = [float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan')]

testdf1 = pd.DataFrame(dates)
testdf1.rename( columns={0:'Date'}, inplace=True )
testdf1['CompanyA'] = values1
testdf1['CompanyB'] = values2

# success case
test1values1 = [float('nan'), 0.0, -0.1, 0.0, 0.1, 0.0, 0.01, 0.0, 1.0, 0.0]
test1values2 = [float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan')]

benchmarkdf1 = pd.DataFrame(dates)
benchmarkdf1.rename(columns={0:'Date'}, inplace=True)
benchmarkdf1 = benchmarkdf1.set_index('Date')
benchmarkdf1['Ticker 1'] = test1values1
benchmarkdf1['Ticker 2'] = test1values2

# fail case
test2values1 = [10000, 10000, 9000, 9000, 9900, 9900, 9999, 9999, 19998, 19998]
test2values2 = [float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan'),float('nan')]

benchmarkdf2 = pd.DataFrame(dates)
benchmarkdf2.rename(columns={0:'Date'}, inplace=True)
benchmarkdf2 = benchmarkdf2.set_index('Date')
benchmarkdf2['Ticker 1'] = test2values1
benchmarkdf2['Ticker 3'] = test2values2

def test_WRANGLE_INTEGRATION(test, benchmark):
  # First Test
  time_index_generator(test)
  
  # Second Test
  anon_tickers = ticker_list_generator(len(test.columns))
  test.columns = anon_tickers
  
  # Third Test
  results = price_to_returns('daily', test) 
  
  pd.testing.assert_frame_equal(results, benchmark)
  print('test successful')

test_WRANGLE_INTEGRATION(testdf1, benchmarkdf1)

# reset testdf1
testdf1 = pd.DataFrame(dates)
testdf1.rename( columns={0:'Date'}, inplace=True )
testdf1['CompanyA'] = values1
testdf1['CompanyB'] = values2

test_WRANGLE_INTEGRATION(testdf1, benchmarkdf2)